{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary \n",
    "\n",
    "The idea here is to exploit some of the work done in earlier projects to obtain an optimal lasso model for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as time\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We define many model evaluation helper functions here (and almost all of them are self explanatory). \n",
    "###### They have been borrowed from the relevant_functions module, which was written for the housing prediction problem (https://www.kaggle.com/c/house-prices-advanced-regression-techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_score(my_model, X, Y):\n",
    "    predictions = my_model.predict(X)\n",
    "    return evaluate_model_score_given_predictions(predictions, Y)\n",
    "\n",
    "def make_predictions(my_model, X):\n",
    "    predictions = my_model.predict(X)\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model_score_given_predictions(predictions, Y):\n",
    "    mean_of_squared_error1 = \\\n",
    "        mean_squared_error((np.log(np.abs(Y + 1))), (np.log(np.abs(predictions + 1))))\n",
    "    return np.sqrt(mean_of_squared_error1)\n",
    "\n",
    "def evaluate_neg_model_score(my_model, X, Y):\n",
    "    return (-1) * evaluate_model_score(my_model, X, Y)\n",
    "\n",
    "\n",
    "def cross_val_scores_given_model(my_model, X, Y, cv=5):\n",
    "    cross_val_score1 = cross_val_score(my_model, \n",
    "                                       X, Y, \n",
    "                                       scoring=evaluate_model_score,\n",
    "                                       cv=cv)\n",
    "    return cross_val_score1\n",
    "\n",
    "def cross_val_score_given_model(my_model, X, Y, cv=5):\n",
    "    cross_val_score1 = cross_val_score(my_model, \n",
    "                                       X, Y,\n",
    "                                       scoring=evaluate_model_score, \n",
    "                                       cv=cv)\n",
    "                                       \n",
    "    return cross_val_score1.mean()\n",
    "\n",
    "def fit_pipeline_and_cross_validate(my_pipeline,\n",
    "                                    train_data,\n",
    "                                    X_columns,\n",
    "                                    Y_column='target'):\n",
    "    X = train_data[X_columns]\n",
    "    Y = train_data[[Y_column]].values.ravel()\n",
    "    my_pipeline.fit(X, Y)\n",
    "    return (my_pipeline, cross_val_score_given_model(my_pipeline, X, Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.071911096572876"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "train_data = pd.read_csv(INPUT_DIR + 'train.csv')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.35545420646667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "test_data = pd.read_csv(INPUT_DIR + 'test.csv')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A pipeline for impelmenting lasso model with normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lasso_pipe_with_scaling(alpha=2000):\n",
    "    my_pipe = make_pipeline(StandardScaler(),linear_model.Lasso(alpha=alpha))\n",
    "    return my_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_COLUMNS = [col for col in train_data.columns if col not in ['ID', 'target']]\n",
    "Y_COLUMN = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are already aware of the data conversion to float and hence we suppress warnings regarding the same.\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHAS = np.concatenate([np.arange(100000, 1500000, 100000), np.arange(50000, 100000, 10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_to_cross_val_score = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360.959095954895"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "for alpha_val in ALPHAS:\n",
    "    (my_pipe, cross_val_score1) = fit_pipeline_and_cross_validate(\n",
    "        get_lasso_pipe_with_scaling(alpha=alpha_val), \n",
    "        train_data, \n",
    "        X_COLUMNS)\n",
    "    alpha_to_cross_val_score[alpha_val] = cross_val_score1\n",
    "time.time() -ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{50000: 2.049342716648238,\n",
       " 60000: 2.0198157269999446,\n",
       " 70000: 2.002283987637136,\n",
       " 80000: 1.977493900486174,\n",
       " 90000: 1.9698114242428697,\n",
       " 100000: 1.9618027160556892,\n",
       " 200000: 1.9544845895654,\n",
       " 300000: 1.96405332141402,\n",
       " 400000: 1.9758551317784125,\n",
       " 500000: 1.9850499883352337,\n",
       " 600000: 1.9918723217614354,\n",
       " 700000: 1.9977906171437056,\n",
       " 800000: 2.0032383786330477,\n",
       " 900000: 2.0083959792986263,\n",
       " 1000000: 2.013499222571732,\n",
       " 1100000: 2.0185609929491095,\n",
       " 1200000: 2.023471759265643,\n",
       " 1300000: 2.028317006226269,\n",
       " 1400000: 2.03311781069742}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_to_cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on test data with the model constructed using the most optimal value of alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipeline_and_make_predictions_on_test_set(my_pipeline,\n",
    "                                                  train_data,\n",
    "                                                  test_data,\n",
    "                                                  X_columns,\n",
    "                                                  Y_column='target'):\n",
    "    X = train_data[X_columns]\n",
    "    Y = train_data[[Y_column]].values.ravel()\n",
    "    my_pipeline.fit(X, Y)\n",
    "    \n",
    "    X_test = test_data[X_columns]\n",
    "    \n",
    "    return (my_pipeline, make_predictions(my_pipeline, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(final_pipeline, predictions_on_test_data) = \\\n",
    "    fit_pipeline_and_make_predictions_on_test_set(get_lasso_pipe_with_scaling(min(alpha_to_cross_val_score, \n",
    "                                                                                  key=alpha_to_cross_val_score.get)), \n",
    "                                                  train_data, \n",
    "                                                  test_data, \n",
    "                                                  X_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['target'] = [np.abs(x) for x in predictions_on_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['ID', 'target']].to_csv('submission_lasso_sklearn2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
